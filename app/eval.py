from datetime import datetime
from typing import List, Dict, Literal, Callable, Union, cast
from loguru import logger

from models_api.generate import llm
from models_api.system_prompt import judge
from utils.database import Database
from utils.config import conf

class llmJudge:
    """
    This class defines an evaluator for responses generated by an LLM.
    It uses another LLM to judge the quality of responses to a query.
    """

    ratings = {
        "task": {
            "generation": ["excellent", "good", "satisfactory", "neutral", "unsatisfactory", "poor", "terrible"], 
            "retrieval": ["very-relevant", "somewhat-relevant", "neutral", "somewhat-irrelevant", "very-irrelevant"]
            },
        }
    rating_point_map = lambda task: {rating:point for rating, point in zip(llmJudge.ratings["task"][task], range(-3, 4))}

    def __init__(self, 
                 task:Literal["retrieval", "generation"], 
                 summarize:Literal["average", "best-of-n"]="average", 
                 experiment:str="test_experiment", 
                 **kwargs):
        logger.info("Initializing LLM Judge")
        self.judge = llm(judge)
        self.task = task
        self.summarize = summarize
        self.experiment = experiment
        self.response_model:str = conf["models"]["llm"]["name"] if self.task == "generation" else ""

    def _evaluate(self, 
                  query_response_pair:str, 
                  evaluator:Callable[[str, str], str], 
                  evaluation_response_schema:Dict, 
                  **kwargs) -> Dict:
        """
        Evaluates the response to a query using the LLM judge.
        :param query_response_pair: A string containing the query and the response to be evaluated.
        :param evaluator: A function that formats the evaluation prompt.
        :param evaluation_response_schema: A dictionary defining the expected schema of the evaluation response.
        :param kwargs: Additional keyword arguments for the LLM judge.
        :return: The evaluation result from the LLM judge.
        """
        logger.debug("Evaluating the responses to query-response pair:\n{}", query_response_pair.replace("'", "\\'").replace('"', '\\"'))
        result = self.judge.generate(
            query_response_pair, 
            formatter=evaluator,
            response_schema=evaluation_response_schema, 
            **kwargs
        )
        result = cast(dict, result)  
        return result

    def evaluate_retrieval_task(self, query:str, retrieval_response:str, **kwargs) -> Dict:
        """
        Evaluates a retrieval task response.
        :param query: The query for which the retrieval was performed.
        :param retrieved_chunks: A list of document chunks retrieved in response to the query.
        :return: The evaluation result from the LLM judge.
        """
        logger.info("Evaluating retrieval task for the following query:\n'{}'", query.replace("'", "\\'").replace('"', '\\"'))
        evaluator = lambda role, query_response_pair: f"""
        Please evaluate the quality of retrieval for the query.
        {query_response_pair}
        Evaluate each retieved chunk.
        """
        evaluation_response_schema = {
            "type": "OBJECT",
            "properties": {
                "retrieval_response": {
                    "type": "ARRAY",
                    "items": {
                        "description": "A retrieved document chunk and its evaluation.",
                        "type": "OBJECT",
                        "properties": {
                            "retrieved_chunk": {
                                "type": "STRING",
                                "description": "A retrieved document chunk."
                            },
                            "rating": {
                                "type": "STRING", 
                                "description": "A rating of retrieval quality.", 
                                "enum": llmJudge.ratings["task"]["retrieval"]
                            },
                            "score": {
                                "type": "NUMBER", 
                                "description": "A score between 0 and 1 (upto 2 decimal places) indicating the quality or relevance of the retieved chunk."
                            }
                        },
                        "required": [
                            "retrieved_chunk",
                            "rating", 
                            "score"
                        ]
                    }
                }
            },
            "required": [
                "retrieval_response"
            ]
        }
        query_response_pair = f"Query: {query}\nRetrieved Chunks:\n{retrieval_response}"
        result = self._evaluate(query_response_pair, evaluator, evaluation_response_schema)
        return self.summarize_results(result["retrieval_response"]) 

    def evaluate_generation_task(self, query:str, response:str, **kwargs) -> Dict:
        """
        Evaluates a generation task response.
        :param query: The query for which the response was generated.
        :param response: The response generated by an LLM.
        :return: The evaluation result from the LLM judge.
        """
        logger.info("Evaluating generation task for the following query:\n'{}'", query.replace("'", "\\'").replace('"', '\\"'))
        evaluator = lambda role, query_response_pair: f"""
        Please evaluate the quality of LLM generated response to the query.
        {query_response_pair}
        """
        evaluation_response_schema = {
            "type": "OBJECT",
            "properties": {
                "rating": {
                    "type": "STRING", 
                    "description": "A rating of the response quality.", 
                    "enum": llmJudge.ratings["task"]["generation"]
                },
                "score": {
                    "type": "NUMBER", 
                    "description": "A score between 0 and 1 (upto 2 decimal places) indicating the quality of the response."
                },
                "reasoning": {
                    "type": "STRING", 
                    "description": "An explanation of the rating and score given to the response."
                }
            },
            "required": [
                "rating", 
                "score", 
                "reasoning"
            ]
        }
        query_response_pair = f"Query: {query}\nResponse: {response}"
        return self._evaluate(query_response_pair, evaluator, evaluation_response_schema)

    def summarize_results(self, results:List[Dict]) -> Dict:
        """
        Summarizes results from evaluations.
        :param results: A list of dictionaries containing evaluation results.
        :return: A dictionary summarizing the overall evaluation.
        """
        average_score = sum(float(result["score"]) for result in results)/ len(results)
        total_rating_points = sum(llmJudge.rating_point_map(self.task)[result["rating"]] for result in results)
        average_points = max(min(round(total_rating_points / len(results), 0), 3), -3)
        average_rating = llmJudge.ratings["task"][self.task][int(average_points) + 3]
        summary = {
                "rating": average_rating,
                "score": round(average_score, 2)
                }
        return summary

    def evaluate(self, queries:List[str], responses:List[Union[List[str], str]], **kwargs) -> Dict:
        """
        Evaluates a list of queries and their corresponding responses.
        :param queries: A list of queries to evaluate.
        :param responses: A list of responses corresponding to the queries.
        :return: A dictionary containing the evaluation results and summary.
        """
        if len(queries) != len(responses):
            raise ValueError("Queries and responses must have the same length.")
        results = []
        for query, response in zip(queries, responses):
            if self.task == "retrieval":
                response = response if isinstance(response, list) else [response]
                retrieval_response = "\n".join([f"{i+1}. {chunk}" for i, chunk in enumerate(response)])
                result = self.evaluate_retrieval_task(query, retrieval_response, **kwargs)
                result["query"] = query
                result["response"] = retrieval_response
                self.persist([result])
            if self.task == "generation":
                response = response if isinstance(response, str) else "\n".join(response)
                result = self.evaluate_generation_task(query, response, **kwargs)
                result["query"] = query
                result["response"] = response
                self.persist([result])
            results.append(result)
        logger.info("Summarizing evaluation results")
        summary = self.summarize_results(results)
        logger.info("{} evaluation results:\n rating: {}\n score: {}", 
                    self.task, 
                    summary.get("rating", "NA"), 
                    summary.get("score", "NA"))
        return summary

    def persist(self, results:List[Dict], table_name:str="eval.evaluated_responses") -> None:
        """
        Persists the evaluation results to a database table.
        :param results: A list of dictionaries containing evaluation results.
        :param table_name: The name of the database table to persist the results.
        """
        logger.debug("Persisting evaluation results to '{}'", table_name)
        db = Database(table_name)
        timestamp = str(datetime.now())
        records = []
        for result in results:
            query = result.get("query", "").replace("'", "`").replace('"', '`')
            response = result.get("response", "").replace("'", "`").replace('"', '`')
            rating = result.get("rating", "")
            score = result.get("score", "")
            reasoning = result.get("reasoning", "").replace("'", "`").replace('"', '`')
            records.append(
                [
                    f"'{self.task}'",
                    f"'{self.experiment}'",
                    f"'{timestamp}'",
                    f"'{self.response_model}'",
                    f"'{query}'",
                    f"'{response}'",
                    f"'{rating}'",
                    f"{score}",
                    f"'{reasoning}'"
                ]
            )
        logger.debug("Inserting records:\n{}", "\n".join([", ".join(record) for record in records]))
        db.records_transaction(records)

    def __call__(self, queries:List[str], responses:List[Union[List[str], str]], **kwargs) -> Dict:
        """
        A wrapper on the evaluate method.
        """
        return self.evaluate(queries, responses, **kwargs)